{"title":"Entropy (information theory)","summary":"In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes. The concept of information entropy was introduced by Claude Shannon in his 1948 paper \"A Mathematical Theory of Communication\",[1][2] and is also referred to as Shannon entropy. As an example, consider a biased coin with probability p of landing on heads and probability 1 âˆ’ p of landing on tails. The maximum surprise is for p = 1/2, when there is no reason to expect one outcome over another. In this case a coin flip has an entropy of one bit. The minimum surprise is for p = 0 or p = 1, when the event is known and the entropy is zero bits. When the entropy is zero bits, this is sometimes referred to as unity, where there is no uncertainty at all - no freedom of choice - no information. Other values of p give different entropies between zero and one bits.","image":"Entropy_flip_2_coins.jpg.webp","url":"Entropy_(information_theory)"}